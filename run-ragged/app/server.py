"""
Service using FastAPI to handle requests and hit RAG pipeline.

This service provides the main API interface for a Retrieval Augmented Generation (RAG) system that
processes and queries PDF documents. It integrates with various Google Cloud Services, Vector Storage,
and Large Language Model (LLM) capabilities to provide intelligent document search and
question-answering capabilities.

Key Components:
    - FastAPI: Web framework for building APIs.
    - Google Cloud Storage (GCS): Stores PDF documents.
    - Cloud SQL (PostgreSQL) with pgvector: Stores document embeddings.
    - Vertex AI: Provides embeddings and LLM capabilities.
    - Prometheus: Collects metrics for monitoring.
    - Circuit Breaker: Implements resilience patterns.
    - Frontend: Serves react UI.
    - Asynchronous Operations: Leverages asyncio for non-blocking I/O operations.
    - Configuration: Manages settings via environment variables.

Environment Variables (Required):
    - DB_INSTANCE_NAME: Cloud SQL instance connection name.
    - DB_USER: Database username.
    - DB_PASS: Database password.
    - DB_NAME: Database name.
    - PDF_BUCKET_NAME: GCS bucket for PDF storage.
    - INDEXER_SERVICE_URL: URL of the indexer service (default: http://indexer:8080).
    - PRE_SHARED_KEY: Pre-shared key for authenticating with the indexer service.

Environment Variables (Optional):
    - CIRCUIT_BREAKER_FAILURE_THRESHOLD: Number of failures before circuit breaker trips (default: 3).
    - CIRCUIT_BREAKER_RECOVERY_TIMEOUT: Time in seconds before circuit breaker recovers (default: 30).
    - LLM_MAX_OUTPUT_TOKENS: Maximum number of tokens generated by the LLM (default: 2048).
    - LLM_MODEL_NAME: Name of the LLM model to use (default: "gemini-1.0-pro-001").
    - LLM_TEMPERATURE: Temperature setting for LLM generation (default: 0.2).
    - GROK_API_KEY: API key for the Grok LLM service if you have one (optional when Grok is disabled)
    - GROK_API_URL: URL for the Grok LLM service (optional when Grok is disabled)
"""

import asyncio
import httpx
import logging.config
import os
import pg8000
import random
import tempfile
import time
import tomllib
from app.ai_collaboration import AICollaborationManager, PersonaManager
from app.conversation_memory import ConversationMemory
from app.debug_metrics import RAGMetrics
from circuitbreaker import CircuitBreaker
from datetime import datetime
from fastapi import FastAPI, UploadFile, HTTPException, Response, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import RedirectResponse, FileResponse, JSONResponse
from google.cloud import storage
from google.cloud.sql.connector import Connector, IPTypes
from langchain_community.vectorstores.pgvector import PGVector
from langchain_core.documents import Document
from langchain_core.prompts import PromptTemplate
from langchain_google_vertexai import VertexAI, VertexAIEmbeddings
from prometheus_client import Counter, Histogram
from pydantic import BaseModel, Field
from sqlalchemy import create_engine, text
from sqlalchemy.exc import DBAPIError
from sqlalchemy.orm import Session
from typing import Optional, Dict, List, Any, Tuple, Final
from app.logging_config import setup_logging

# Project name for theme/persona
with open("pyproject.toml", "rb") as f:  # binary required to parse file as UTF-8 with universal newlines disabled
    project_metadata = tomllib.load(f)
project_name = project_metadata["tool"]["poetry"]["name"]
project = project_name.title()  # Albear / Twig / Etc

# Environment-based configuration
CIRCUIT_BREAKER_FAILURE_THRESHOLD: Final = int(os.getenv('CIRCUIT_BREAKER_FAILURE_THRESHOLD', 3))
CIRCUIT_BREAKER_RECOVERY_TIMEOUT: Final = int(os.getenv('CIRCUIT_BREAKER_RECOVERY_TIMEOUT', 30))
LLM_MAX_OUTPUT_TOKENS: Final = int(os.getenv('LLM_MAX_OUTPUT_TOKENS', 2048))
LLM_MODEL_NAME: Final = str(os.getenv('LLM_MODEL_NAME', "gemini-1.0-pro-001"))
LLM_TEMPERATURE: Final = float(os.getenv('LLM_TEMPERATURE', 0.2))
PRE_SHARED_KEY: Final = str(os.getenv("PRE_SHARED_KEY"))

# Globals
metrics = RAGMetrics() # Initialize metrics for RAG performance tracking
metrics_vectorstore = None # Initialize metrics vectorstore as None so we can update it in startup
conversation_memory = None
collaboration_manager = None

# Initialize logging with appropriate level
setup_logging()
logger = logging.getLogger(__name__)

# Initialize FastAPI application with Cross-Origin Resource Sharing (CORS) settings.
app = FastAPI(
    title="RAG API Service",
    docs_url="/api" # This is important since the default is /docs which conflicts with sphinx
)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Allow all origins for testing. In production, specify the origins
    allow_credentials=True,
    allow_methods=["*"], # Allow all methods
    allow_headers=["*"], # Allow all headers
)

def cosine_similarity(v1: List[float], v2: List[float]) -> float:
    """Compute cosine similarity between two vectors."""
    dot_product = sum(x * y for x, y in zip(v1, v2))
    norm1 = sum(x * x for x in v1) ** 0.5
    norm2 = sum(x * x for x in v2) ** 0.5
    return dot_product / (norm1 * norm2) if norm1 and norm2 else 0.0

class ServiceCircuitBreaker:
    """Circuit breaker implementation for external service dependencies."""
    def __init__(self):
        """Initializes circuit breakers for database, storage and vertex services"""
        failure_threshold = CIRCUIT_BREAKER_FAILURE_THRESHOLD
        recovery_timeout = CIRCUIT_BREAKER_RECOVERY_TIMEOUT
        self.db_breaker = CircuitBreaker(failure_threshold=failure_threshold, recovery_timeout=recovery_timeout)
        self.storage_breaker = CircuitBreaker(failure_threshold=failure_threshold, recovery_timeout=recovery_timeout)
        self.vertex_breaker = CircuitBreaker(failure_threshold=failure_threshold, recovery_timeout=recovery_timeout)

# Initialize circuit breakers
circuit_breakers = ServiceCircuitBreaker()

# Initialize database connector for Cloud SQL
connector = Connector()

class DatabaseManager:
    """Manages database operations and connections."""
    def __init__(self):
        """Initializes the DatabaseManager."""
        logger.info("Initializing DatabaseManager")
        self._startup_lock = asyncio.Lock()
        self._indexing_lock = asyncio.Lock()
        self.is_indexing = False

    def _get_connection_params(self) -> Dict[str, str]:
        """Retrieves database connection parameters from environment variables."""
        required_params = ["DB_USER", "DB_PASS", "DB_NAME", "DB_INSTANCE_NAME"]
        params = {}
        for param in required_params:
            value = os.getenv(param)
            if not value:
                raise ValueError(f"Missing required environment variable: {param}")
            params[param] = value
        return params

    def get_sync_connection(self):
        """Gets a synchronous database connection."""
        params = self._get_connection_params()
        return connector.connect(
            instance_connection_string=params["DB_INSTANCE_NAME"],
            driver="pg8000",
            user=params["DB_USER"],
            password=params["DB_PASS"],
            db=params["DB_NAME"],
            ip_type=IPTypes.PUBLIC,
        )

    async def get_vectorstore(self):
        """Creates PGVector instance."""
        logger.info("Starting get_vectorstore")
        try:
            params = self._get_connection_params()
            
            # Test basic connectivity first
            conn = self.get_sync_connection()
            cursor = conn.cursor()
            try:
                # Check pgvector extension
                cursor.execute("SELECT extname FROM pg_extension WHERE extname = 'vector'")
                if not cursor.fetchone():
                    logger.error("pgvector extension not found")
                    return None
                
                # Check document_embeddings table
                cursor.execute("SELECT EXISTS (SELECT 1 FROM document_embeddings LIMIT 1)")
                logger.info("Successfully tested database connection and tables")
                
            finally:
                cursor.close()
                conn.close()

            # Create connection string for Cloud SQL using pg8000
            connection_string = f"postgresql+pg8000://"  # Minimal connection string
            
            # Create engine args for Cloud SQL
            engine_args = {
                "creator": lambda: self.get_sync_connection()
            }
            
            logger.info(f"Initializing vectorstore for Cloud SQL instance: {params['DB_INSTANCE_NAME']}")

            # Initialize vectorstore with engine arguments and optimized search config
            vectorstore = PGVector(
                collection_name="document_embeddings",
                connection_string=connection_string,
                embedding_function=VertexAIEmbeddings(model_name="textembedding-gecko@003"),
                engine_args=engine_args,
                distance_strategy="cosine",  # Explicit distance strategy
                collection_metadata={"hnsw:ef_search": 128}  # Tune search parameters
            )

            # Verify vectorstore works
            try:
                test_result = await vectorstore.asimilarity_search("test", k=1)
                logger.info(f"Successfully verified vectorstore functionality")
                return vectorstore
            except Exception as e:
                logger.error(f"Failed to verify vectorstore: {str(e)}")
                return None

        except Exception as e:
            logger.error(f"Failed to create vectorstore: {str(e)}", exc_info=True)
            return None

# Initialize Prometheus metrics to monitor the service's performance and health.
REQUEST_COUNT = Counter('http_requests_total', 'Total HTTP requests', ['method', 'endpoint', 'status_code'])
REQUEST_LATENCY = Histogram('http_request_duration_seconds', 'HTTP request latency', ['method', 'endpoint'])
PDF_PROCESSING_LATENCY = Histogram('pdf_processing_duration_seconds', 'PDF processing latency')
QUERY_LATENCY = Histogram('query_duration_seconds', 'Query processing latency')
INDEXER_CONNECTION_LATENCY = Histogram(
    'indexer_connection_latency_seconds',
    'Indexer service connection latency'
)
RETRIEVAL_ERRORS = Counter(
    'retrieval_errors_total',
    'Total retrieval errors by type',
    ['error_type']
)

async def verify_vectorstore() -> Tuple[bool, str]:
    """
    Verifies the vector store connection and functionality.

    Returns:
        Tuple[bool, str]: A tuple containing:
            - bool: True if verification successful, False otherwise
            - str: Description of the verification result or error message
    """
    try:
        # Test basic connection
        vectorstore = await DatabaseManager().get_vectorstore()
        if not vectorstore:
            return False, "Vectorstore not properly initialized"

        if not hasattr(vectorstore, 'asimilarity_search'):
            return False, "Vectorstore not properly initialized"

        # Test basic connection
        logger.info("Starting vector store verification test...")
        test_results = await vectorstore.asimilarity_search("test", k=1)
        result_count = len(test_results)

        # Get total document count
        doc_count = 0
        try:
            with Session(DatabaseManager().engine) as session:
                result = session.execute(text("SELECT COUNT(*) FROM document_embeddings"))
                doc_count = result.scalar()
        except Exception as e:
            logger.error(f"Failed to get document count: {e}")
            return False, f"Failed to get document count: {e}"

        logger.info(f"Vectorstore verification complete: {result_count} test results, {doc_count} total documents")

        if doc_count == 0:
            return False, "Vector store is empty - no documents indexed"

        return True, f"Vectorstore operational with {doc_count} documents"

    except Exception as e:
        error_msg = f"Vectorstore verification failed: {str(e)}"
        logger.error(error_msg, exc_info=True)
        return False, error_msg

class IndexerServiceManager:
    """
    Manages the connection and health checks for the indexer service.
    This class encapsulates the logic to communicate with the external indexer service.
    """
    def __init__(self, url: str, max_retries: int = 3):
        """
        Initializes the IndexerServiceManager.
        Args:
            url (str): The base URL of the indexer service.
            max_retries (int): Maximum number of retries for the health check (default is 3).
        """
        self.url = url
        self.max_retries = max_retries
        self.client = httpx.AsyncClient(timeout=10.0)
        self._last_health_check: Optional[datetime] = None
        self._health_check_interval = int(os.getenv("INDEXER_HEALTH_CHECK_INTERVAL", 60))  # seconds
        self._health_check_lock = asyncio.Lock() # Lock for health check to prevent race condition

    async def needs_health_check(self) -> bool:
        """
        Checks if a health check is needed based on the elapsed time since the last check.

        Returns:
            bool: True if a health check is needed, False otherwise.
        """
        if not self._last_health_check:
            return True
        elapsed = (datetime.utcnow() - self._last_health_check).total_seconds()
        return elapsed >= self._health_check_interval

    async def verify_connection(self) -> Tuple[bool, Optional[str]]:
        """
        Verifies the connection to the indexer service and provides a detailed status.
         Returns:
             Tuple[bool, Optional[str]]: A tuple containing:
               - bool: True if connection is healthy, False otherwise.
               - Optional[str]: Error message if connection fails, else None.
        """
        max_retries = 3
        retry_delay = 1 # seconds

        async with self._health_check_lock:
            for attempt in range(max_retries):
                try:
                    start_time = time.time()
                    headers = {"X-Pre-Shared-Key": PRE_SHARED_KEY} if PRE_SHARED_KEY else None
                    async with httpx.AsyncClient(timeout=30.0) as client:
                        try:
                            logger.info(f"Attempting health check at {self.url}/health, attempt {attempt + 1}")
                            response = await client.get(f"{self.url}/health", headers=headers)
                            response.raise_for_status()
                            response_json = response.json()

                            latency = time.time() - start_time
                            INDEXER_CONNECTION_LATENCY.observe(latency)

                            logger.info(f"Indexer health check response: {response.status_code}, {response_json}, latency: {latency:.2f}s")

                            # Only check status now
                            is_healthy = (
                                response.status_code == 200 and
                                response_json.get("status") == "healthy"
                            )

                            if is_healthy:
                                self._last_health_check = datetime.utcnow()
                                return True, None

                            error_message = f"Unhealthy response: Status={response.status_code}, Content={response_json}"
                            logger.error(error_message)
                            if attempt < max_retries - 1:
                                sleep_duration = retry_delay * (2**attempt) + random.uniform(0, 0.1)
                                logger.info(f"Retrying in {sleep_duration:.2f} seconds")
                                await asyncio.sleep(sleep_duration)
                            continue
                        except httpx.RequestError as e:
                            logger.error(f"Request to indexer failed: {str(e)}", exc_info=True)
                            if attempt < max_retries - 1:
                                sleep_duration = retry_delay * (2**attempt) + random.uniform(0, 0.1)
                                logger.info(f"Retrying in {sleep_duration:.2f} seconds")
                                await asyncio.sleep(sleep_duration)
                            continue

                except Exception as e:
                    logger.error(f"Unexpected error in indexer connection: {str(e)}", exc_info=True)
                    return False, f"Connection failed: {str(e)}"

            logger.error(f"Indexer health check failed after {max_retries} attempts")
            return False, "Indexer health check failed after multiple attempts"
        
class CollaborationSettings(BaseModel):
    """Settings for controlling collaborative AI interactions."""
    enable_grok: bool = True # enable grok service
    max_conversation_turns: int = 3 # Max turns before memory reset.
    synthesis_temperature: float = 0.3 # temperature of synthesis LLM model
    debug_mode: bool = Field(default=False, description="Enable detailed debug information")

class QueryRequest(BaseModel):
    """Request model for incoming queries."""
    query: str # the user query string
    collaboration_settings: CollaborationSettings = Field(default_factory=CollaborationSettings) # Configurable collaboration settings.

class ErrorDetail(BaseModel):
    """Model for error details in responses."""
    type: str # Type of error
    stage: str # Stage where error occurred.
    message: str # Error message.
    timestamp: str # timestamp of error
    debug_context: Optional[Dict] = None # Optional debugging context.

class FeedbackRequest(BaseModel):
    """Model for feedback requests."""
    message_id: int # ID of the original message
    feedback_value: str # user feedback
    source_context: Optional[dict] = None # context of feedback (optional)
    metadata: Optional[dict] = None # Additional metadata about the feedback

class QueryResponse(BaseModel):
    """Model for query responses."""
    answer: str # Answer generated by the system.
    metadata: Optional[Dict] = None # metadata about the response
    error: Optional[ErrorDetail] = None # Error detail, if any.
    debug_info: Optional[Dict] = None # Optional debug information.

class MetricsTrackingVectorStore:
    """
    A wrapper around a vector store to track metrics during retrieval.
    This class intercepts similarity search calls to measure retrieval performance.
    """
    def __init__(self, vectorstore: PGVector, metrics: RAGMetrics):
        """
        Initializes the MetricsTrackingVectorStore.
        Args:
            vectorstore (PGVector): The underlying vector store instance.
            metrics (RAGMetrics): An instance of RAGMetrics for tracking metrics.
        """
        self.vectorstore = vectorstore
        self.metrics = metrics
        if not self.vectorstore:
           logger.error(f"Error! Vector store not properly initialized")
           raise Exception("Vectorstore is Null")

        if not self.vectorstore.embedding_function:
           logger.error(f"Error! Vector store embedding function not available! Type is {type(self.vectorstore.embedding_function)}", exc_info=True)
           raise Exception("Vectorstore Embedding Function is Null")
        logger.info(f"MetricsTrackingVectorStore created, embedding_function: {self.vectorstore.embedding_function}")

    @classmethod
    async def create(cls, db_manager: DatabaseManager, metrics: RAGMetrics):
        """
        Factory method to create instance asynchronously.
        Args:
            db_manager (DatabaseManager): Database manager instance
            metrics (RAGMetrics): Metrics tracker instance
        Returns:
            Optional[MetricsTrackingVectorStore]: New instance or None if creation fails
        """
        try:
            vectorstore = await db_manager.get_vectorstore()
            if not vectorstore:
                logger.error("Failed to get vectorstore in create()")
                return None
            return cls(vectorstore, metrics)
        except Exception as e:
            logger.error(f"Failed to create MetricsTrackingVectorStore: {str(e)}")
            return None

    async def asimilarity_search(self, query, k=6):
        """Enhanced similarity search with proper vector formatting."""
        with self.metrics.track_phase("similarity_search"):
            query_embedding = await self.vectorstore.embedding_function.aembed_query(query)
            vector_str = f"[{','.join(str(x) for x in query_embedding)}]"

            db_manager = DatabaseManager()
            conn = db_manager.get_sync_connection()
            try:
                cursor = conn.cursor()
                try:
                    cursor.execute("""
                        SELECT content, doc_metadata, embedding <=> %s::vector as similarity
                        FROM document_embeddings
                        WHERE embedding IS NOT NULL
                        ORDER BY similarity
                        LIMIT %s
                    """, (vector_str, k))
                    
                    results = cursor.fetchall()
                    return [
                        Document(
                            page_content=row[0],
                            metadata=row[1]
                        ) for row in results
                    ]
                finally:
                    cursor.close()
            finally:
                conn.close()

logger.info("Checking vectorstore initialization...")
try:
    vectorstore =  DatabaseManager().get_vectorstore()
    logger.info(f"Vectorstore type: {type(vectorstore)}")
    logger.info(f"Vectorstore attributes: {dir(vectorstore)}")
    logger.info("Vectorstore initialization complete")
except Exception as e:
    logger.error(f"Error checking vectorstore: {str(e)}", exc_info=True)

# Initialize conversation memory
try:
    vectorstore = DatabaseManager().get_vectorstore()
    conversation_memory = ConversationMemory(
        vectorstore=vectorstore,
        enable_persistence=True, # Save conversation history to vectorstore
        max_turns=50 # Set maximum conversation turns
    )
except Exception as e:
    logger.error(f"Failed to instantiate conversation memory: {str(e)}", exc_info=True)
    conversation_memory=None

# Initialize Large Language Model (LLM)
llm = VertexAI(
    model_name=LLM_MODEL_NAME,
    temperature=LLM_TEMPERATURE,
    max_output_tokens=LLM_MAX_OUTPUT_TOKENS,
    top_k=40, # TODO: document this
    top_p=0.95 # TODO: document this
)

async def format_docs_with_metrics(docs, metrics_tracker=None):
    """
    Formats retrieved documents with metrics tracking.

    Args:
        docs (List[Document]): List of documents to be formatted.
        metrics_tracker (Optional[RAGMetrics]): Metrics tracker instance

    Returns:
        Optional[str]: Formatted string of documents or None if no documents
    """
    if not docs:
        return None
    try:
        # If we have a metrics tracker, track this phase
        if metrics_tracker:
            with metrics_tracker.track_phase("context_formatting"):
                metrics_tracker.log_debug_info(
                    "formatting_documents",
                    doc_count=len(docs),
                    sources=[doc.metadata.get('source', 'Unknown') for doc in docs]
                )

        formatted = "\n\n".join(
            f"[Source: {doc.metadata.get('source', 'Unknown')}, "
            f"Page: {doc.metadata.get('page', 'Unknown')}]\n{doc.page_content}"
            for doc in docs
        )

        if not formatted.strip():
            if metrics_tracker:
                metrics_tracker.log_debug_info("empty_context_warning")
            logger.warning("Formatted context is empty")
            return None

        # Track successful formatting
        if metrics_tracker:
            metrics_tracker.log_debug_info(
                "formatting_complete",
                formatted_length=len(formatted),
                doc_count=len(docs)
            )

        logger.info(f"Successfully formatted {len(docs)} documents")
        return formatted

    except Exception as e:
        if metrics_tracker:
            metrics_tracker.log_debug_info(
                "formatting_error",
                error=str(e)
            )
        logger.error(f"Error formatting documents: {str(e)}")
        return None

async def preprocess_query(query: str) -> str:
    """
    Preprocesses a query string for better retrieval results.
    This method cleans and standardizes the query before sending it to the vector store.

    Args:
        query (str): The original query string.

    Returns:
        str: The processed query string.
    """
    # Remove extra whitespace
    query = " ".join(query.split())

    # Basic cleaning
    query = query.strip().lower()

    # Remove common stop words if very long query
    # Removed stop word filter.
    return query

# Configure retriever for similarity search
try:
    if metrics_vectorstore: # Added this check
      vectorstore = DatabaseManager().get_vectorstore()
      retriever = metrics_vectorstore.as_retriever(
          search_type="similarity", # Use similarity search
          search_kwargs={"k": 6} # Return top 6 results
      )
    else:
        retriever = None
        logger.error(f"Retriever could not be instantiated since metrics_vectorstore is None")
except Exception as e:
    logger.error(f"Error creating retriever: {str(e)}", exc_info=True)
    retriever = None

# Define system prompt template for LLM interactions.
# Initialize persona manager with project name
persona_manager = PersonaManager(project=project)
persona = persona_manager.get_persona_prompt()
prompt_template = PromptTemplate.from_template(
"""
{persona}

Given the following relevant context:
{context}

Answer the following question:
{query}

Remember to base your response primarily on the provided context.
If the context doesn't contain enough information, say so explicitly.
Please provide citations for all information, and return an answer in a single paragraph.

Answer:
""")

# Initialize collaboration manager with persona_manager
try:
    collaboration_manager = AICollaborationManager(
        conversation_memory=conversation_memory,
        gemini_llm=llm,
        grok_api_key=os.getenv("GROK_API_KEY"),
        prompt_template=prompt_template,
        db_manager=DatabaseManager(), # call here (before it was db_manager)
        vectorstore=metrics_vectorstore,
        persona_manager=persona_manager  # Pass the initialized persona_manager
    )
except Exception as e:
    logger.error(f"Failed to initialize AICollaborationManager: {str(e)}", exc_info=True)
    collaboration_manager = None

# Startup event handler
@app.on_event("startup")
async def startup_event():
    """
    Event handler that runs when the application starts up.
    Initializes all core components and verifies connections.
    """
    global metrics_vectorstore, conversation_memory, collaboration_manager
    
    indexer_url = os.getenv("INDEXER_SERVICE_URL")
    logger.info(f"Starting server with indexer URL: {indexer_url}")
    if not indexer_url:
        logger.warning("INDEXER_SERVICE_URL not set! Using default URL")
        indexer_url = "http://indexer:8080"

    # Verify required environment variables
    required_env_vars = ["DB_USER", "DB_PASS", "DB_NAME", "DB_INSTANCE_NAME", "PRE_SHARED_KEY"]
    missing_vars = [var for var in required_env_vars if not os.getenv(var)]
    if missing_vars:
        raise ValueError(f"Missing required environment variables: {', '.join(missing_vars)}")

    try:
        # Initialize database manager
        db_manager = DatabaseManager()
        
        # Initialize vectorstore first
        logger.info("Initializing vectorstore")
        vectorstore = await db_manager.get_vectorstore()
        if not vectorstore:
            raise Exception("Failed to initialize vectorstore")
        logger.info("Successfully initialized vectorstore")
        
        # Create metrics tracking vectorstore
        logger.info("Creating metrics tracking vectorstore")
        metrics_vectorstore = await MetricsTrackingVectorStore.create(db_manager, metrics)
        if not metrics_vectorstore:
            raise Exception("Failed to create metrics vectorstore")
        logger.info("Successfully created metrics vectorstore")
        
        # Initialize conversation memory
        logger.info("Initializing conversation memory")
        conversation_memory = ConversationMemory(
            vectorstore=vectorstore,
            enable_persistence=True,
            max_turns=50
        )
        
        # Initialize collaboration manager
        logger.info("Initializing collaboration manager")
        collaboration_manager = AICollaborationManager(
            conversation_memory=conversation_memory,
            gemini_llm=llm,
            grok_api_key=os.getenv("GROK_API_KEY"),
            prompt_template=prompt_template,
            db_manager=db_manager,
            vectorstore=metrics_vectorstore,
            persona_manager=persona_manager
        )
            
        # Initialize indexer manager
        app.state.indexer_manager = IndexerServiceManager(indexer_url)
        success, error = await app.state.indexer_manager.verify_connection()
        if not success:
            logger.warning(f"Initial indexer connection failed: {error}")
        else:
            logger.info("Successfully connected to indexer service")

        # Store components in app state
        app.state.db_manager = db_manager
        app.state.metrics_vectorstore = metrics_vectorstore
        app.state.conversation_memory = conversation_memory
        app.state.collaboration_manager = collaboration_manager

        logger.info("Startup sequence completed successfully")

    except Exception as e:
        logger.error(f"Error during startup: {str(e)}", exc_info=True)
        raise  # Re-raise to fail startup

############ FastAPI

@app.get("/")
async def redirect_root_to_ui():
    """
    Root endpoint redirects to the web UI.
    Ensures users accessing the root URL are directed to the application interface.
    """
    return RedirectResponse("/ui")

# UI Route handlers
@app.get("/ui/{full_path:path}")
async def serve_spa(full_path: str = ""):
    """
    Serves the Single Page Application (SPA) frontend.
    Handles all UI routes by serving the main index.html.
    """
    # First check if this is an assets request
    if full_path.startswith("assets/"):
        file_path = full_path.replace("assets/", "")
        # Try both public and dist directories
        file_locations = [
            f"frontend/dist/assets/{file_path}",
            f"frontend/public/assets/{file_path}"
        ]

        for file_location in file_locations:
            if os.path.isfile(file_location):
                content_type = None
                if file_path.endswith('.js'):
                    content_type = 'application/javascript'
                elif file_path.endswith('.css'):
                    content_type = 'text/css'
                elif file_path.endswith('.jpg') or file_path.endswith('.jpeg'):
                    content_type = 'image/jpeg'
                elif file_path.endswith('.png'):
                    content_type = 'image/png'

                return FileResponse(
                    file_location,
                    media_type=content_type,
                    filename=os.path.basename(file_path)
                )

        return Response(status_code=404)

    # If not an assets request, serve index.html
    index_path = "frontend/dist/index.html"
    if not os.path.exists(index_path):
        logger.error(f"UI file not found: {index_path}")
        raise HTTPException(status_code=404, detail="UI not found")
    return FileResponse(index_path)

@app.post("/query", response_model=QueryResponse)
async def query_documents(request: QueryRequest):
    """
    Endpoint to handle incoming user queries.
    This route is responsible for orchestrating the RAG pipeline, including vector search,
    LLM processing, and collaboration.

    Args:
        request (QueryRequest): The incoming query and settings.

    Returns:
        QueryResponse: The response containing the answer, metadata, debug information, and potential errors.
    """
    if metrics_vectorstore is None:
            return JSONResponse(
                    status_code=200,
                    content={
                        "answer": "I encountered a system error. Please try again.",
                        "error": ErrorDetail(
                            type="system_error",
                            stage="initialization",
                            message="MetricsTrackingVectorStore failed to instantiate",
                            timestamp=datetime.utcnow().isoformat(),
                            debug_context=None
                        ).dict(),
                        "debug_info": None
                    }
            )

    with metrics_vectorstore.metrics.track_phase("request_processing"):
        try:
            logger.info(f"Received query: {request.query}")
            logger.debug(f"Debug mode is set to: {request.collaboration_settings.debug_mode}") # Added this line to double check debug flag
            logging.getLogger().debug("Testing root logger for debug output") # Test root logger

            # Step 1: Verify indexer health if needed
            if await app.state.indexer_manager.needs_health_check():
                success, error = await app.state.indexer_manager.verify_connection()
                if not success:
                    logger.warning(f"Indexer health check failed: {error}")

            # Step 2: Attempt vector search first
            try:
                logger.debug(f"Preprocessing query: {request.query}")
                query = await preprocess_query(request.query)
                logger.debug(f"Preprocessed query: {query}")

                logger.debug(f"Query before vector search: {query}")
                 # Log the query *after* preprocessing
                docs = await metrics_vectorstore.asimilarity_search(
                    query=query,
                    k=6,
                )
                if docs:
                    logger.debug(f"Source metadata before vector search: {docs[0].metadata}")  # log metadata before vector search

                # Format documents if found
                raw_context = await format_docs_with_metrics(docs, metrics_vectorstore.metrics) if docs else None

            except Exception as e:
                logger.error(f"Vector search failed: {str(e)}")
                docs = []
                raw_context = None

                return JSONResponse(
                    status_code=200,
                    content={
                        "answer": "I encountered an error during retrieval. Please try again.",
                        "error": ErrorDetail(
                            type="retrieval_error",
                            stage="vector_search",
                            message=str(e),
                            timestamp=datetime.utcnow().isoformat(),
                            debug_context=metrics_vectorstore.metrics.get_metrics() if request.collaboration_settings.debug_mode else None
                        ).dict(),
                        "debug_info": metrics_vectorstore.metrics.get_metrics() if request.collaboration_settings.debug_mode else None
                    }
                )

            # Step 3: Prepare metadata for response tracking
            query_metadata = {
                "debug_mode": request.collaboration_settings.debug_mode,
                "source": docs[0].metadata.get("source") if docs else None,
                "request_timestamp": datetime.utcnow().isoformat(),
                 "retrieval_stats": {
                    "chunks": len(docs) if docs else 0,
                    "sources": list(set(doc.metadata.get("source", "unknown")
                                    for doc in docs)) if docs else []
                }
            }
            logger.debug(f"Metadata before collaboration: {query_metadata}")

            # Step 4: Process through collaboration manager
            try:
              response = await collaboration_manager.process_query(
                  query=request.query,
                  raw_context=raw_context,
                  metadata=query_metadata
              )
              logger.debug(f"LLM response prior to return: {response}") # Log prior to return
              await app.state.conversation_memory.add_interaction(
                  query=request.query,
                  response=response.content, # Use response.content, not the whole response object
                  raw_context=raw_context, # Pass raw_context if available
                  metadata=query_metadata # Pass metadata if needed
              )
            except Exception as e:
                logger.error(f"LLM generation failed: {str(e)}")
                return JSONResponse(
                    status_code=200,
                    content={
                        "answer": "I encountered an error during generation. Please try again.",
                        "error": ErrorDetail(
                            type="generation_error",
                            stage="llm_generation",
                            message=str(e),
                            timestamp=datetime.utcnow().isoformat(),
                            debug_context=metrics_vectorstore.metrics.get_metrics() if request.collaboration_settings.debug_mode else None
                        ).dict(),
                        "debug_info": metrics_vectorstore.metrics.get_metrics() if request.collaboration_settings.debug_mode else None
                    }
                )

            #Correct the response type for debug purposes
            response_source = response.metadata.get("collaboration", {}).get("source") or "gemini"

            return QueryResponse(
                answer=response.content,
                metadata={
                    "processing_time": metrics_vectorstore.metrics.phases["request_processing"].duration,
                    "source": response_source,
                     "llm_response": response.metadata.get("llm_response"),
                    **response.metadata
                },
                debug_info=metrics_vectorstore.metrics.get_metrics() if request.collaboration_settings.debug_mode else None
            )

        except Exception as e:
            logger.error(f"Query processing failed: {str(e)}", exc_info=True)
            error_detail = ErrorDetail(
                type="system_error",
                stage="query_processing",
                message=str(e),
                timestamp=datetime.utcnow().isoformat(),
                debug_context=metrics_vectorstore.metrics.get_metrics() if request.collaboration_settings.debug_mode else None
            )

            return JSONResponse(
                status_code=200,
                content={
                    "answer": "I encountered an error processing your query. Please try again.",
                    "error": error_detail.dict(),
                    "debug_info": metrics_vectorstore.metrics.get_metrics() if request.collaboration_settings.debug_mode else None
                }
            )

@app.post("/feedback")
async def process_feedback(feedback: FeedbackRequest):
    """
    Endpoint to process user feedback.
    Proxies the feedback to the indexer service.

    Args:
        feedback (FeedbackRequest): User feedback data.

    Returns:
        dict: Response from the indexer service.
    """
    try:
        indexer_url = os.getenv("INDEXER_SERVICE_URL", "http://indexer:8080")
        headers = {"X-Pre-Shared-Key": PRE_SHARED_KEY} if PRE_SHARED_KEY else None
        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{indexer_url.rstrip('/')}/feedback",
                json=feedback.dict(),
                headers=headers
            )
            response.raise_for_status()
            return response.json()
    except Exception as e:
        logger.error(f"Feedback processing failed: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/feedback/analytics")
async def get_feedback_analytics():
    """
    Endpoint to retrieve feedback analytics from the indexer service.

    Returns:
        dict: Analytics data from the indexer service.
    """
    try:
        indexer_url = os.getenv("INDEXER_SERVICE_URL", "http://indexer:8080")
        headers = {"X-Pre-Shared-Key": PRE_SHARED_KEY} if PRE_SHARED_KEY else None
        async with httpx.AsyncClient() as client:
            response = await client.get(
                f"{indexer_url.rstrip('/')}/feedback/analytics",
                headers=headers
            )
            response.raise_for_status()
            return response.json()
    except Exception as e:
        logger.error(f"Failed to get feedback analytics: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health_check():
    """Health check endpoint for API server."""
    db_manager = DatabaseManager() # Instantiate DatabaseManager here
    conn = None # Initialize conn to None
    try:
       # Quick DB connection check using get_sync_connection from DatabaseManager
        conn = db_manager.get_sync_connection() # Get sync connection
        cursor = conn.cursor() # Get a cursor
        cursor.execute("SELECT 1") # Execute a simple query
        cursor.fetchone() # Fetch one result to check if query worked
        cursor.close() # Close the cursor
        conn.close() # Close the connection

        health_status = {
            "status": "healthy",
            "db_connected": True,
            "initialization_status": "unknown",
            "last_init_time": datetime.utcnow().isoformat(),
            "project_name": project_name
        }
        print(f"DEBUG: project_name in /health: {project_name}") # Debug line
        return health_status

    except Exception as e:
         if conn: # Ensure connection is closed in case of error
             conn.close()
         return {
            "status": "unhealthy",
            "error": str(e),
            "initialization_status": "unknown"
         }

@app.get("/indexer/health")
async def proxy_indexer_health():
    """
    Proxies the /health endpoint from the indexer service.
    This endpoint provides the same health information as the indexer service.
    """
    try:
        indexer_url = os.getenv("INDEXER_SERVICE_URL", "http://indexer:8080")
        headers = {"X-Pre-Shared-Key": PRE_SHARED_KEY} if PRE_SHARED_KEY else None
        async with httpx.AsyncClient() as client:
            response = await client.get(
                f"{indexer_url}/health",
                headers=headers
            )
            response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)
            return response.json()
    except httpx.HTTPError as e:
        logger.error(f"Error proxying indexer /health: {e}")
        raise HTTPException(status_code=e.response.status_code, detail=str(e))
    except Exception as e:
        logger.error(f"Error proxying indexer /health: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to proxy indexer /health: {str(e)}")

@app.get("/indexer/status")
async def proxy_indexer_status():
    """
    Proxies the /status endpoint from the indexer service.
    This endpoint provides the indexing status from the indexer service.
    """
    try:
        indexer_url = os.getenv("INDEXER_SERVICE_URL", "http://indexer:8080")
        headers = {"X-Pre-Shared-Key": PRE_SHARED_KEY} if PRE_SHARED_KEY else None
        async with httpx.AsyncClient() as client:
            response = await client.get(
                f"{indexer_url}/status",
                headers=headers
            )
            response.raise_for_status()
            return response.json()
    except httpx.HTTPError as e:
        logger.error(f"Error proxying indexer /status: {e}")
        raise HTTPException(status_code=e.response.status_code, detail=str(e))
    except Exception as e:
        logger.error(f"Error proxying indexer /status: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to proxy indexer /status: {str(e)}")

@app.get("/indexer/health/detailed")
async def proxy_indexer_detailed_health():
    """
    Proxies the /health/detailed endpoint from the indexer service.
    This endpoint provides detailed health information about the indexer service.
    """
    try:
        indexer_url = os.getenv("INDEXER_SERVICE_URL", "http://indexer:8080")
        headers = {"X-Pre-Shared-Key": PRE_SHARED_KEY} if PRE_SHARED_KEY else None
        async with httpx.AsyncClient() as client:
            response = await client.get(
                f"{indexer_url}/health/detailed",
                headers=headers
            )
            response.raise_for_status()
            return response.json()
    except httpx.HTTPError as e:
         logger.error(f"Error proxying indexer /health/detailed: {e}")
         raise HTTPException(status_code=e.response.status_code, detail=str(e))
    except Exception as e:
        logger.error(f"Error proxying indexer /health/detailed: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to proxy indexer /health/detailed: {str(e)}")

@app.get("/debug/metrics")
async def get_debug_metrics():
    """
    Endpoint to retrieve debug metrics and system status.
    Provides detailed information about the state of different system components and their performance.

    Returns:
        dict: A dictionary containing metrics, system stats and a timestamp
    """
    metrics = RAGMetrics()
    vectorstore =  DatabaseManager().get_vectorstore()
    with metrics.track_phase("system_check"):
        # Check various system components
        components_status = {
            "gemini": await check_gemini_status(),
            "grok": await check_grok_status(),
            "vectorstore": await check_vectorstore_status(),
            "conversation_memory": bool(conversation_memory)
        }

        # Get system stats
        stats = {
            "indexed_documents": await get_document_count(vectorstore=vectorstore),
            "average_response_time": REQUEST_LATENCY.describe()["mean"],
            "requests_per_minute": REQUEST_COUNT._value.rate(),
            "components": components_status
        }

        return {
            "metrics": metrics.get_metrics(),
            "system_stats": stats,
            "timestamp": datetime.utcnow().isoformat()
        }

async def check_gemini_status():
    """
    Checks the status of the Gemini LLM by making a test request.
    Returns:
        dict: A dictionary indicating the status and latency of Gemini.
    """
    try:
        response = await llm.agenerate(["test"])
        return {"status": "healthy", "latency": response.llm_output.get("latency")}
    except Exception as e:
        return {"status": "error", "message": str(e)}

async def check_grok_status():
    """
    Checks the status of the Grok LLM service.

    Returns:
        dict: A dictionary indicating the status of the Grok service.
    """
    if not collaboration_manager.grok:
        return {"status": "disabled"}
    try:
        # TODO: Add Grok health check logic
        return {"status": "healthy"}
    except Exception as e:
        return {"status": "error", "message": str(e)}

async def check_vectorstore_status():
    """
    Checks the status of the vector store.
    Performs a test similarity search to check if the vector store is functioning correctly

    Returns:
        dict: A dictionary indicating the status of the vector store and connection details.
    """
    try:
        # Perform test query
        vectorstore =  DatabaseManager().get_vectorstore()
        await vectorstore.asimilarity_search("test", k=1)

        # Get pool statistics if available
        pool_stats = None
        try:
            pool = vectorstore._Session.kw['bind'].pool
            pool_stats = {
                "pool_size": pool.size(),
                "checked_out": pool.checkedin(),
                "overflow": pool.overflow()
            }
        except Exception as e:
            logger.warning(f"Could not get pool stats: {e}")

        return {
            "status": "healthy",
            "pool_stats": pool_stats
        }
    except Exception as e:
        return {
            "status": "error",
            "message": str(e),
            "pool_stats": None
        }

async def get_document_count(vectorstore) -> int:
    """
    Retrieves the number of indexed documents in the vector store.
    Returns:
        int: The count of indexed documents or 0 on failure
    """
    try:
        # Add your document counting logic
        return await vectorstore.get_document_count()
    except Exception:
        return 0

@app.get("/files")
@circuit_breakers.storage_breaker
async def list_files():
    """
    Lists all PDF files stored in the GCS bucket.
    This route interacts directly with Google Cloud Storage.

    Returns:
        dict: A dictionary containing the list of PDF files.
    """
    try:
        bucket_name = os.getenv("PDF_BUCKET_NAME")
        if not bucket_name:
            raise HTTPException(status_code=500, detail="PDF_BUCKET_NAME environment variable not set")

        storage_client = storage.Client()
        bucket = storage_client.bucket(bucket_name)

        files = []
        for blob in bucket.list_blobs():
            if blob.name.lower().endswith('.pdf'):
                files.append({
                    "name": blob.name,
                    "size": blob.size,
                    "created": blob.time_created.isoformat(),
                    "updated": blob.updated.isoformat()
                })

        return {"files": files}
    except Exception as e:
        logger.error(f"Failed to list files: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/upload")
@circuit_breakers.storage_breaker
async def upload_pdf(file: UploadFile):
    """Handles PDF file uploads to Google Cloud Storage."""
    if not file.filename.lower().endswith('.pdf'):
        raise HTTPException(status_code=400, detail="Only PDF files are accepted")

    start_time = datetime.now()
    try:
        bucket_name = os.getenv("PDF_BUCKET_NAME")
        if not bucket_name:
            raise HTTPException(status_code=500, detail="PDF_BUCKET_NAME environment variable not set")

        logger.info(f"Starting upload of {file.filename}")
        storage_client = storage.Client()
        bucket = storage_client.bucket(bucket_name)
        blob = bucket.blob(file.filename)

        # Check if the blob exists
        if blob.exists():
            logger.warning(f"Blob {file.filename} already exists, not overwriting")
            return {
              "message": f"File {file.filename} already exists, not overwriting.",
              "status": "already_exists",
              "filename": file.filename
            }

        # This upload will stream the file directly
        with tempfile.NamedTemporaryFile(delete=False) as temp_file:
            try:
                contents = await file.read()
                temp_file.write(contents)
                temp_file_path = temp_file.name
                logger.info(f"Downloaded {len(contents)} bytes to local file {temp_file_path}")

                # Upload from local path
                blob.upload_from_filename(temp_file_path, content_type='application/pdf')
                logger.info(f"Successfully uploaded {file.filename} to GCS, size = {blob.size}")
            except Exception as e:
              logger.error(f"Failed to upload {file.filename} to GCS: {e}", exc_info=True)
              return {
                "message": f"File upload failed: {str(e)}",
                "status": "failed_upload",
                "filename": file.filename
              }
            finally:
              os.unlink(temp_file_path)

        # Trigger indexing with better error handling
        indexer_url = os.getenv("INDEXER_SERVICE_URL", "http://indexer:8080")
        headers = {"X-Pre-Shared-Key": PRE_SHARED_KEY} if PRE_SHARED_KEY else None
        async with httpx.AsyncClient(timeout=30.0) as client:
            try:
                logger.info(f"Triggering indexing for {file.filename}")
                response = await client.post(
                    f"{indexer_url}/index",
                    json={"files": [file.filename]},
                    headers=headers
                )
                response.raise_for_status()
                indexing_result = response.json()
                logger.info(f"Indexing triggered successfully: {indexing_result}")
            except Exception as e:
                logger.error(f"Indexing of new file failed: {str(e)}", exc_info=True)
                return {
                    "message": f"File uploaded but indexing failed: {str(e)}",
                    "status": "partial_success",
                    "filename": file.filename
                }

        return {
            "message": f"Successfully uploaded and triggered indexing for {file.filename}",
            "status": "success",
            "filename": file.filename
        }
    except Exception as e:
        logger.error(f"Upload failed: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        PDF_PROCESSING_LATENCY.observe((datetime.now() - start_time).total_seconds())

@app.delete("/files/{filename}")
@circuit_breakers.storage_breaker
async def delete_file(filename: str):
    """
    Completely removes a file and all its associated data from the system.
    Performs a thorough cleanup of all database records and vector store entries.
    """
    try:
        bucket_name = os.getenv("PDF_BUCKET_NAME")
        if not bucket_name:
            raise HTTPException(status_code=500, detail="PDF_BUCKET_NAME environment variable not set")

        logger.info(f"Starting complete deletion process for {filename}")

        # 1. Delete from GCS
        try:
            storage_client = storage.Client()
            bucket = storage_client.bucket(bucket_name)
            blob = bucket.blob(filename)
            blob.delete()
            logger.info(f"Deleted file {filename} from GCS")
        except Exception as e:
            logger.error(f"GCS deletion failed for {filename}: {e}")
            raise

        # 2. Complete database cleanup
        try:
            # Use a standard session
            with Session(DatabaseManager().engine) as session:
                # First get all content hashes associated with this file
                result = session.execute(
                    text("""
                        SELECT doc_metadata->>'content_hash' as hash
                        FROM document_embeddings
                        WHERE doc_metadata->>'source' = :filename
                    """),
                    {"filename": filename}
                )
                content_hashes = [row[0] for row in result if row[0]]

                if content_hashes:
                    logger.info(f"Found {len(content_hashes)} content hashes to clean up for {filename}")

                    # Delete all entries with these content hashes
                    session.execute(
                        text("""
                            DELETE FROM document_embeddings
                            WHERE doc_metadata->>'content_hash' = ANY(:hashes)
                        """),
                        {"hashes": content_hashes}
                    )

                # Delete all embeddings for this source
                session.execute(
                    text("""
                        DELETE FROM document_embeddings
                        WHERE doc_metadata->>'source' = :filename
                    """),
                    {"filename": filename}
                )

                # Also cleanup any orphaned entries
                session.execute(
                    text("""
                        DELETE FROM document_embeddings
                        WHERE doc_metadata->>'source' IS NULL
                        OR doc_metadata->>'content_hash' IS NULL
                    """)
                )

                session.commit()
                logger.info(f"Completed database cleanup for {filename}")

        except Exception as e:
            logger.error(f"Database cleanup failed for {filename}: {e}")
            raise

        # 3. Clean up vector store
        try:
            # First try source-based deletion
            vectorstore = DatabaseManager().get_vectorstore()
            vectorstore.delete({"source": filename})

            # Then try content hash based deletion if we have hashes
            if content_hashes:
                for hash_value in content_hashes:
                    vectorstore.delete({"content_hash": hash_value})

            logger.info(f"Completed vector store cleanup for {filename}")
        except Exception as e:
            logger.error(f"Vector store cleanup failed for {filename}: {e}", exc_info=True)
            # Continue since we already cleaned the database

        # 4. Verify cleanup
        try:
            with Session(DatabaseManager().engine) as session:
                remaining = session.execute(
                    text("""
                        SELECT COUNT(*)
                        FROM document_embeddings
                        WHERE doc_metadata->>'source' = :filename
                    """),
                    {"filename": filename}
                ).scalar()

                if remaining > 0:
                    logger.warning(f"Found {remaining} remaining entries for {filename} after cleanup")
                else:
                    logger.info(f"Verified complete cleanup of {filename}")
        except Exception as e:
            logger.error(f"Cleanup verification failed: {e}")

        return {
            "message": f"Successfully deleted {filename} and all associated data",
            "status": "success",
            "details": {
                "gcs_deleted": True,
                "hashes_cleaned": len(content_hashes) if content_hashes else 0,
                "verified": remaining == 0 if 'remaining' in locals() else None
            }
        }
    except Exception as e:
        logger.error(f"Delete operation failed: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/files/reindex")
async def reindex_documents():
    """
    Proxy endpoint to trigger document reindexing on the indexer service.
    This route sends a reindexing command to the indexer.

    Returns:
         JSONResponse: Result of reindexing command proxied from indexer service
    """
    try:
        indexer_url = os.getenv("INDEXER_SERVICE_URL", "http://indexer:8080")
        headers = {"X-Pre-Shared-Key": PRE_SHARED_KEY} if PRE_SHARED_KEY else None
        logger.info(f"Indexer URL: {indexer_url}")

        # Add detailed error logging
        async with httpx.AsyncClient(timeout=60.0) as client:  # Increased timeout
            try:
                response = await client.post(f"{indexer_url}/index", json={}, headers=headers) # We are sending empty json
                logger.info(f"Indexer response status: {response.status_code}")
                logger.info(f"Indexer response content: {response.text}")
                response.raise_for_status()
                return response.json()
            except httpx.HTTPError as e:
                logger.error(f"HTTP Error: {str(e)}")
                logger.error(f"Response content: {e.response.text if hasattr(e, 'response') else 'No response'}")
                raise
    except Exception as e:
        logger.error(f"Reindexing failed: {str(e)}", exc_info=True)
        return JSONResponse(
            status_code=500,
            content={
                "message": f"Reindexing failed: {str(e)}",
                "status": "error",
            }
        )

@app.middleware("http")
async def metrics_middleware(request: Request, call_next):
    """Middleware for collecting request metrics."""
    start_time = datetime.now()
    logger.info(f"Request path: {request.url.path}")

    response = await call_next(request)
    logger.info(f"Response status: {response.status_code}")

    duration = (datetime.now() - start_time).total_seconds()

    REQUEST_COUNT.labels(
        method=request.method,
        endpoint=request.url.path,
        status_code=response.status_code
    ).inc()

    REQUEST_LATENCY.labels(
        method=request.method,
        endpoint=request.url.path
    ).observe(duration)

    return response

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8080)